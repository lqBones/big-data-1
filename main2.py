# big_data_pipeline.py
import os
import logging
import dask.dataframe as dd
import numpy as np
import pandas as pd
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
DATA_DIR = Path("data")
RAW_DATA_PATTERN = DATA_DIR / "raw" / "transactions_*.parquet"
ANALYTICS_OUTPUT = DATA_DIR / "analytics" / "summary.json"
N_ROWS = 100_000_000  # 100 –º–ª–Ω —Å—Ç—Ä–æ–∫
N_PARTITIONS = 200    # –ë–æ–ª—å—à–µ –ø–∞—Ä—Ç–∏—Ü–∏–π = –ª—É—á—à–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
CATEGORIES = ["food", "tech", "travel", "entertainment"]

def generate_large_dataset():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (Big Data) –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet."""
    logger.info("–ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    (DATA_DIR / "raw").mkdir(parents=True, exist_ok=True)
    (DATA_DIR / "analytics").mkdir(parents=True, exist_ok=True)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è "–ª–µ–Ω–∏–≤–æ–≥–æ" Dask DataFrame –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ø–∞–º—è—Ç—å
    df = dd.from_map(
        lambda i: pd.DataFrame({
            'user_id': np.random.randint(1, 2_000_000, size=N_ROWS // N_PARTITIONS),
            'amount': np.random.exponential(scale=50.0, size=N_ROWS // N_PARTITIONS),  # –ë–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            'category': np.random.choice(CATEGORIES, size=N_ROWS // N_PARTITIONS, p=[0.4, 0.2, 0.25, 0.15])
        }),
        range(N_PARTITIONS),
        divisions=[None] * (N_PARTITIONS + 1)
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Big Data
    output_path = str(DATA_DIR / "raw" / "transactions_*.parquet")
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Parquet: {output_path}")
    df.to_parquet(output_path, compression="snappy", write_index=False)
    logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

def analyze_data():
    """–ê–Ω–∞–ª–∏–∑ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö: –∞–≥—Ä–µ–≥–∞—Ü–∏—è, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Parquet...")
    df = dd.read_parquet(RAW_DATA_PATTERN)

    logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏...")
    summary = (
        df.groupby("category")
        .agg(
            total_amount=("amount", "sum"),
            avg_amount=("amount", "mean"),
            transaction_count=("amount", "count"),
            unique_users=("user_id", "nunique")
        )
        .round(2)
        .compute()  # –ó–∞–ø—É—Å–∫ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
    )

    logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏...")
    summary.to_json(ANALYTICS_OUTPUT, orient="index", indent=2)
    logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {ANALYTICS_OUTPUT}")

    # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
    print("\nüìä –°–≤–æ–¥–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    print(summary)

if __name__ == "__main__":
    # –®–∞–≥ 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑)
    if not list((DATA_DIR / "raw").glob("*.parquet")):
        generate_large_dataset()
    
    # –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑
    analyze_data()